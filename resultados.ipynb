{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fb784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos Dataset 1: ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Naive Bayes', 'XGBoost', 'Gradient Boosting', 'Ensemble (Voting)', 'Ensemble (Weighted)']\n",
      "Modelos Dataset 2: ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'Naive Bayes', 'XGBoost', 'Gradient Boosting', 'Ensemble (Voting)', 'Ensemble (Weighted)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xd-bo\\AppData\\Local\\Temp\\ipykernel_4744\\3143752648.py:291: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(df_f1.index, rotation=45, ha=\"right\", fontsize=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gráficos gerados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Carregar resultados dos modelos clássicos\n",
    "# ============================================================\n",
    "\n",
    "results_ds1 = pd.read_csv(\"resultados_dataset1_final.csv\", index_col=0)\n",
    "results_ds2 = pd.read_csv(\"resultados_dataset2_final.csv\", index_col=0)\n",
    "\n",
    "# Garante que não tenha linha de GPT misturada\n",
    "mask_gpt1 = ~results_ds1.index.str.contains(\"gpt\", case=False)\n",
    "mask_gpt2 = ~results_ds2.index.str.contains(\"gpt\", case=False)\n",
    "results_ds1 = results_ds1[mask_gpt1]\n",
    "results_ds2 = results_ds2[mask_gpt2]\n",
    "\n",
    "print(\"Modelos Dataset 1:\", results_ds1.index.tolist())\n",
    "print(\"Modelos Dataset 2:\", results_ds2.index.tolist())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Resultados do GPT (K-Fold) - preencha se mudar\n",
    "# ============================================================\n",
    "\n",
    "gpt_metrics = {\n",
    "    \"accuracy\": 0.9492,\n",
    "    \"precision\": 0.7326,\n",
    "    \"recall\": 0.9786,\n",
    "    \"f1\": 0.8379\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Gráfico geral de F1: todos os modelos (sem GPT)\n",
    "# ============================================================\n",
    "\n",
    "def plot_global_f1(results_ds1, results_ds2, filename=\"visao_geral_modelos.png\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        \"Dataset 1\": results_ds1[\"f1\"],\n",
    "        \"Dataset 2\": results_ds2[\"f1\"]\n",
    "    }).sort_values(\"Dataset 1\", ascending=False)\n",
    "\n",
    "    x = np.arange(len(data))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar(x - width/2, data[\"Dataset 1\"], width, label=\"Dataset 1\", edgecolor=\"black\")\n",
    "    ax.bar(x + width/2, data[\"Dataset 2\"], width, label=\"Dataset 2\", edgecolor=\"black\")\n",
    "\n",
    "    ax.set_ylabel(\"F1-Score\", fontweight=\"bold\")\n",
    "    ax.set_title(\"Comparação de F1-Score entre Modelos e Datasets\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(data.index, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_global_f1(results_ds1, results_ds2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Top 5 modelos por F1 (por dataset)\n",
    "# ============================================================\n",
    "\n",
    "def plot_top5(results, dataset_name, filename):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "    top5 = results.nlargest(5, \"f1\")\n",
    "    colors = plt.cm.YlOrBr(np.linspace(0.4, 0.9, len(top5)))\n",
    "\n",
    "    ax.barh(range(len(top5)), top5[\"f1\"], color=colors, edgecolor=\"black\")\n",
    "    ax.set_yticks(range(len(top5)))\n",
    "    ax.set_yticklabels(top5.index)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"F1-Score\")\n",
    "    ax.set_title(f\"Top 5 Modelos – {dataset_name}\")\n",
    "\n",
    "    for i, v in enumerate(top5[\"f1\"]):\n",
    "        ax.text(v + 0.002, i, f\"{v:.4f}\", va=\"center\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_top5(results_ds1, \"Dataset 1\", \"top5_dataset1.png\")\n",
    "plot_top5(results_ds2, \"Dataset 2\", \"top5_dataset2.png\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Trade-off Tempo vs F1 (por dataset)\n",
    "# ============================================================\n",
    "\n",
    "def plot_tradeoff(results, dataset_name, filename):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "    sc = ax.scatter(results[\"time\"], results[\"f1\"],\n",
    "                    s=150,\n",
    "                    c=np.arange(len(results)),\n",
    "                    cmap=\"viridis\",\n",
    "                    edgecolor=\"black\",\n",
    "                    alpha=0.8)\n",
    "\n",
    "    for name, row in results.iterrows():\n",
    "        ax.annotate(name, (row[\"time\"], row[\"f1\"]),\n",
    "                    fontsize=7, ha=\"left\", va=\"bottom\")\n",
    "\n",
    "    ax.set_xlabel(\"Tempo total de CV (s)\", fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"F1-Score\", fontweight=\"bold\")\n",
    "    ax.set_title(f\"Trade-off Tempo vs Performance – {dataset_name}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_tradeoff(results_ds1, \"Dataset 1\", \"tradeoff_dataset1.png\")\n",
    "plot_tradeoff(results_ds2, \"Dataset 2\", \"tradeoff_dataset2.png\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Métricas completas por modelo (Accuracy, Precision, Recall, F1)\n",
    "# ============================================================\n",
    "\n",
    "def plot_metrics_by_model(results, dataset_name, filename):\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "    metrics = results[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].copy()\n",
    "    metrics = metrics.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "    idx = np.arange(len(metrics))\n",
    "    width = 0.18\n",
    "\n",
    "    ax.bar(idx - 1.5*width, metrics[\"accuracy\"], width, label=\"Accuracy\")\n",
    "    ax.bar(idx - 0.5*width, metrics[\"precision\"], width, label=\"Precision\")\n",
    "    ax.bar(idx + 0.5*width, metrics[\"recall\"], width, label=\"Recall\")\n",
    "    ax.bar(idx + 1.5*width, metrics[\"f1\"], width, label=\"F1-Score\")\n",
    "\n",
    "    ax.set_xticks(idx)\n",
    "    ax.set_xticklabels(metrics.index, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_ylim(0.7, 1.0)\n",
    "    ax.set_title(f\"Métricas por Modelo – {dataset_name}\")\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_metrics_by_model(results_ds1, \"Dataset 1\", \"metricas_dataset1.png\")\n",
    "plot_metrics_by_model(results_ds2, \"Dataset 2\", \"metricas_dataset2.png\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Gráfico só do GPT (médias do K-Fold)\n",
    "# ============================================================\n",
    "\n",
    "def plot_gpt_metrics(gpt_metrics, filename=\"gpt_kfold_metrics.png\"):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    nomes = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "    valores = [gpt_metrics[\"accuracy\"],\n",
    "               gpt_metrics[\"precision\"],\n",
    "               gpt_metrics[\"recall\"],\n",
    "               gpt_metrics[\"f1\"]]\n",
    "\n",
    "    ax.bar(nomes, valores, edgecolor=\"black\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Desempenho do GPT (K-Fold)\")\n",
    "\n",
    "    for i, v in enumerate(valores):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_gpt_metrics(gpt_metrics)\n",
    "\n",
    "def plot_metrics_heatmap(results, dataset_name, filename=\"heatmap_metricas.png\",\n",
    "                         gpt_metrics=None, gpt_label=\"GPT-4o (K-Fold)\"):\n",
    "    \"\"\"\n",
    "    Gera um heatmap de métricas para os modelos clássicos.\n",
    "    Se gpt_metrics for fornecido, adiciona o GPT como mais uma linha.\n",
    "    \"\"\"\n",
    "    metrics = results[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].copy()\n",
    "\n",
    "    # Se quiser incluir o GPT neste heatmap\n",
    "    if gpt_metrics is not None:\n",
    "        metrics.loc[gpt_label] = [\n",
    "            gpt_metrics[\"accuracy\"],\n",
    "            gpt_metrics[\"precision\"],\n",
    "            gpt_metrics[\"recall\"],\n",
    "            gpt_metrics[\"f1\"],\n",
    "        ]\n",
    "\n",
    "    metrics = metrics.round(3)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(metrics, annot=True, cmap=\"YlGnBu\", linewidths=0.5, fmt=\".3f\")\n",
    "    plt.title(f\"Heatmap de Métricas – {dataset_name}\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Modelos\")\n",
    "    plt.xlabel(\"Métricas\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11. Curva ROC dos modelos clássicos (somente modelos com predict_proba)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curves(models_dict, X_test, y_test, filename=\"roc_curves.png\", title=\"Curvas ROC\"):\n",
    "    \"\"\"\n",
    "    models_dict: {\"Nome do Modelo\": modelo_treinado}\n",
    "    Apenas modelos com método predict_proba funcionarão.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Converte rótulos para 0/1\n",
    "    y_true = (y_test == \"spam\").astype(int)\n",
    "\n",
    "    for name, model in models_dict.items():\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_scores = model.decision_function(X_test)\n",
    "        else:\n",
    "            continue  # ignora modelos sem score contínuo\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Aleatório (AUC = 0.5)\")\n",
    "    plt.xlabel(\"Taxa de Falsos Positivos (FPR)\")\n",
    "    plt.ylabel(\"Taxa de Verdadeiros Positivos (TPR)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Comparação GPT vs Melhor Ensemble (Dataset 1)\n",
    "# ============================================================\n",
    "\n",
    "def plot_gpt_vs_best_ensemble(results_ds1, gpt_metrics,\n",
    "                              filename=\"gpt_vs_ensemble.png\"):\n",
    "    # pega melhor ensemble por F1 no Dataset 1\n",
    "    mask_ens = results_ds1.index.str.contains(\"Ensemble\")\n",
    "    ens_results = results_ds1[mask_ens]\n",
    "    best_ensemble_name = ens_results[\"f1\"].idxmax()\n",
    "    best_ensemble = ens_results.loc[best_ensemble_name]\n",
    "\n",
    "    modelos = [best_ensemble_name, \"GPT-4o (K-Fold)\"]\n",
    "    metrics_names = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ens_vals = [best_ensemble[m] for m in metrics_names]\n",
    "    gpt_vals = [gpt_metrics[m] for m in metrics_names]\n",
    "\n",
    "    ax.bar(x - width/2, ens_vals, width, label=best_ensemble_name, edgecolor=\"black\")\n",
    "    ax.bar(x + width/2, gpt_vals, width, label=\"GPT-4o (K-Fold)\", edgecolor=\"black\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Comparação: Melhor Ensemble vs GPT\")\n",
    "\n",
    "    for i, v in enumerate(ens_vals):\n",
    "        ax.text(i - width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=7)\n",
    "    for i, v in enumerate(gpt_vals):\n",
    "        ax.text(i + width/2, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=7)\n",
    "\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_gpt_vs_best_ensemble(results_ds1, gpt_metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Comparação F1 – todos os modelos + GPT (Dataset 1)\n",
    "# ============================================================\n",
    "\n",
    "def plot_all_models_with_gpt(results_ds1, gpt_metrics,\n",
    "                             filename=\"todos_modelos_com_gpt.png\"):\n",
    "    df = results_ds1.copy()\n",
    "    # tempo do GPT não foi medido aqui, colocamos NaN\n",
    "    df.loc[\"GPT-4o (K-Fold)\"] = {\n",
    "        \"accuracy\": gpt_metrics[\"accuracy\"],\n",
    "        \"precision\": gpt_metrics[\"precision\"],\n",
    "        \"recall\": gpt_metrics[\"recall\"],\n",
    "        \"f1\": gpt_metrics[\"f1\"],\n",
    "        \"roc_auc\": np.nan,\n",
    "        \"time\": np.nan\n",
    "    }\n",
    "\n",
    "    df_f1 = df[\"f1\"].sort_values(ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.bar(df_f1.index, df_f1.values, edgecolor=\"black\")\n",
    "    ax.set_ylabel(\"F1-Score\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_title(\"Comparação de F1-Score – Modelos Clássicos + GPT (Dataset 1)\")\n",
    "    ax.set_xticklabels(df_f1.index, rotation=45, ha=\"right\", fontsize=8)\n",
    "\n",
    "    for i, v in enumerate(df_f1.values):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_all_models_with_gpt(results_ds1, gpt_metrics)\n",
    "\n",
    "plot_metrics_heatmap(\n",
    "    results_ds1,\n",
    "    \"Dataset 1\",\n",
    "    \"heatmap_dataset1.png\",\n",
    "    gpt_metrics=gpt_metrics,           \n",
    "    gpt_label=\"GPT-4o (K-Fold)\"\n",
    ")\n",
    "\n",
    "plot_metrics_heatmap(\n",
    "    results_ds2,\n",
    "    \"Dataset 2\",\n",
    "    \"heatmap_dataset2.png\",\n",
    "    gpt_metrics=gpt_metrics,           \n",
    "    gpt_label=\"GPT-4o (K-Fold)\"\n",
    ")\n",
    "\n",
    "plot_roc_curves(\n",
    "    modelos_treinados_ds1,   # dicionário com os modelos já treinados\n",
    "    X_test_ds1,\n",
    "    y_test_ds1,\n",
    "    filename=\"roc_dataset1.png\",\n",
    "    title=\"Curvas ROC – Dataset 1\"\n",
    ")\n",
    "\n",
    "plot_roc_curves(\n",
    "    modelos_treinados_ds2,\n",
    "    X_test_ds2,\n",
    "    y_test_ds2,\n",
    "    filename=\"roc_dataset2.png\",\n",
    "    title=\"Curvas ROC – Dataset 2\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nGráficos gerados com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
